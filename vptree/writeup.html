<!DOCTYPE HTML>
<html>
    <head>
        <meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
        <title>VP Tree</title>


        <link rel="stylesheet" type="text/css" href="poststyle.css" />

        <script src="jquery-1.10.1.min.js"></script>
        <script src="asideScript.js"></script>

        <link rel="stylesheet" href="highlight.css">
        <script src="highlight.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
    </head>

    <body id="top">
        <div class="page sidebar">
            <div class="content">
                <h1>VP Tree</h1>

                <!-- <span name="label"></span> -->
                <!-- <aside name="label"><p>Text.</p></aside> -->
                
                <h2>Introduction</h2>

                <p>We will explore a new data structure called Vantage Point Trees, for space partitioning problems. VP trees are more specifically metric trees, which are trees with the ability to efficiently partition data in n-dimensional metric space. </p>

                <p>Advantages of the VP tree are in performing range queries on a dataset, for example, doing nearest neighbor search. A more well known data structure for KNN search is the KD tree, and we will discuss the pros and cons of using a VP tree for a similar purpose.</p>

                <h2>Construction</h2>

                <p>To illustrate how a VP tree works, we will consider a dataset of 1000 points on a 2D plane.</p>

                <img class="postImage" src="Images/data1.png" alt="Img"></img>

                <p>Each node of the VP tree stores 5 pieces of information:</p>
                <ul>
                    <li><i>data</i> - A list of data points it contains</li>
                    <li><i>vp</i> - A vantage point chosen from <i>data</i></li>
                    <li><i>mu</i> - A radius value defining the range of the node</li>
                    <li><i>inside</i> - The left subtree</li>
                    <li><i>outside</i> - The right subtree</li>
                </ul>

                <p>We will now construct the root node. The <i>data</i> of the root will contain all of the points. A <i>vp</i> is randomly chosen from <i>data</i>. Choosing a good vantage point has a direct impact on the efficiency of the tree, but for simplicity, we will randomly choose a point. </p>

                <p>Once the <i>vp</i> is chosen, we will calculate <i>mu</i>. The <i>mu</i> value represents the radius at which half the <i>data</i> is inside the radius, and half is out. To illustrate the root node, the blue point is the <i>vp</i>, and around it is a circle of radius <i>mu</i>.</p>

                <img class="postImage" src="Images/data2.png" alt="Img"></img>
                
                <p>Now we will subdivide the points. We will create the <i>inside</i> subtree, with its <i>data</i> containing all the points inside of the radius. And we will create the <i>outside</i> subtree with the points outside the radius.</p>

                <img class="postImage" src="Images/data3.png" alt="Img"></img>

                <p>The tree then recursively partitions the remaining space, subvidiving the points in each node. This continues until the subtrees contain no points. Each successive node represents an ever smaller subset of the dataset. For 1000 nodes, we would only need to subdivide 9 times, because the root would have 2 subtrees of 2<sup>9</sup> = 512 points. Here is an illustration of a fully subdivided vantage point tree. </p>

                <img class="postImage" src="Images/data4.png" alt="Img"></img>


                <h2>Construction Analysis</h2>

                <p>At each level of recursion, the tree splits the original problem into two subproblems of half the size. This is because the inside and outside subtrees both contain exacly half the nodes of the parent. At every node, the median of the node's <i>data</i> must be calculated to get a <i>mu</i> value. This is O(n) for the entire level.</p>

                <p>In doing so, the tree has O(logn) depth because it makes binary splits. Therefore, the overall construction complexity of a VP tree is O(nlogn).</p>

                <h2>Searching</h2>

                <p>The whole point of building a space partitioning tree is to perform efficient search queries on it. The goal is to search as few nodes as possible to find neighbors of the query point.</p>

                <p>Consider a circle of radius <i>tau</i> around the query point that encloses all of its nearest neighbors. Suppose we are searching for <i>k</i> nearest neighbors, then <i>tau</i> would contain the closest <i>k</i> points.

                <p>Let's examine this example on <i>k=3</i> nearest neighbors:
                    <ul>
                        <li>14 data points (black points)</li>
                        <li>Query point (red point)</li>
                        <li>Vantage point (blue point)</li>
                        <li>Mu (blue circle)</li>
                        <li>Tau (red circle)</li>
                    </ul>
                </p>

                <b>Case 1:</b>

                <img class="postImage" src="Images/testout.png" alt="Img"></img>

                <p>Notice that <i>tau</i> lies entirely outside of <i>mu</i>. That means the nearest neighbors to the query point lies entire in the <i>outside</i> subtree, and that is the only subtree we need to search. Therefore we can safely ignore the <i>inside</i> subtree, and prune half the work.</p>

                <b>Case 2:</b>

                <img class="postImage" src="Images/testin.png" alt="Img"></img>

                <p>Notice that <i>tau</i> is entirely within <i>mu</i>. This is the opposite of the previous case. We only need to search <i>inside</i>, and can prune the <i>outside</i> subtree.</p>

                <b>Case 3</b>

                <img class="postImage" src="Images/testboth.png" alt="Img"></img>

                <p>Notice that <i>tau</i> intersects <i>mu</i>. We can no longer prune a subtree, and must search both the <i>inside</i> and <i>outside</i> subtrees to find neighbors.</p>

                <p>While searching through the tree, <i>tau</i> can be thought of as the area of interest around the query point. Starting at infinity, we steadily shrink <i>tau</i> by pruning nodes of the tree that are outside the area of interest. We iterate by searching the inside/outside subtrees where they overlap the area of interest. Here is my javascript implementation of the knn algorithm on a VP tree.</p>
                <pre><code class="javascript">// Given root node, k nearest neighbors, and query point.
function knn (root, k, query) {
    // Start search at the root
    var tau = Infinity;
    var toSearch = [root];

    // Store results in priority queue of size k
    // Ordered by distance to query
    var results = new DistanceQueue(query, k); 

    while (toSearch.length > 0) {
        // Pop an element to search.
        var currentNode = toSearch.splice(0, 1)[0];
        var dist = query.dist(currentNode.vp);

        // Node is within area of interest, add node, decrease tau.
        if (dist < tau) {
            toSearch.push(currentNode.vp);
            var farthest = results.last();
            tau = query.dist(farthest);
        }

        // Some of tau is inside mu, must check inside
        if (dist < currentNode.mu + tau) 
            toSearch.push(currentNode.left);

        // Some of tau is outside mu, must check outside
        if (dist >= currentNode.mu - tau)
            toSearch.push(currentNode.right);
    }

    return results;
}</code></pre>

                <h2>Searching Analysis</h2>

                <p>VP trees partition data in a similar way to KD trees. Both select median values to split subsets of data on. KD trees select a dimension to split on, and partition with a hyperplane through the median. VP trees instead split the data with a hypersphere of radius <i>mu</i>. Here is an illustration of the two trees representing the same dataset.</p>

                <img class="postImage" src="Images/data5.png" alt="Img"></img>

                <p>To compare their search time on KNN, each tree was built 5 times on a random dataset, and 10 random queries were run for each build. The test cases were run on various values of <i>n</i> points in the dataset, <i>d</i> dimensions, and <i>k</i> nearest neighbors.</p>

                <h4>Test 1</h4>
                <ul>
                    <li><i>d</i> = 2</li>
                    <li><i>k</i> = 1</li>
                </ul>

                <img class="postImage" src="Images/d2k1.png" alt="Img"></img>

                <p>Nearest neighbor search shows that the two trees perform similarly on low dimensional data. VP trees perform generally worse here because overlapping <i>mu</i> values cause additional searching.</p>

                <h4>Test 2</h4>
                <ul>
                    <li><i>d</i> = 5</li>
                    <li><i>k</i> = 1</li>
                </ul>

                <img class="postImage" src="Images/d5k1.png" alt="Img"></img>

                <p>Let's try increasing the number of dimensions to 5. The average number nodes searched per query increases, but the two trees still display similar trends.</p>

                <h4>Test 3</h4>
                <ul>
                    <li><i>d</i> = 15</li>
                    <li><i>k</i> = 1</li>
                </ul>

                <img class="postImage" src="Images/d15k1.png" alt="Img"></img>

                <p>Let's try some high dimensional data. 15 dimensions show a clear advantage for VP trees. Note that a line of slope 1 is the worse case because that means the tree is searching every node. KD trees are approaching this threshold for high dimensions and lose their logarithmic behavior. VP trees however, are still able to search without going through the whole dataset.</p>

                <h4>Test 4</h4>
                <ul>
                    <li><i>d</i> = 5</li>
                    <li><i>k</i> = 1000</li>
                </ul>

                <img class="postImage" src="Images/d5k1000.png" alt="Img"></img>

                <p>Recall test 2 with <i>d</i> = 5, and <i>k</i> = 1, where KD trees were performing far better. What happens when we increase <i>k</i>? Searching for more nearest neighbors shows an advantage for VP trees. An interesting consequence of using hyperspheres for partitioning is that the <i>inside</i> subtree contains all the points closest to the <i>vp</i>. This is conveniently helpful for large nearest neighbor searches, where the nearest neighbors will be clustered within a sphere around the query.

                <h2>Conclusion</h2>

                <p>Vantage point trees are an unusual binary space partitioning structure, but they provide some advanages over their counterpart, KD trees. On higher dimensional data, we see improvements in search time for the VP tree. </p>

                <p>Certain areas of machine learning, especially image processing, make heavy use of VP trees. Images can be thought as high dimensional datapoints, where each pixel, or each feature can be a dimension. VP trees can then be used for a reverse image search, finding the closest matches to any given image, and other applications.</p>

                <p>An interesting consequence of the distanced based partitioning vs dimension based partitioning is that this tree is not limited to Euclidean space, and works on data in any metric space. The only requirement is that a distance function can be provided that satisfies the triangle inequality. </p>

                <p>Applying this consequence to strings, we can use VP trees to find nearest string matches. Each datapoint in the tree can represent a string, and the distance function can be chosen to be, for example, Levenshtein distance. This partitions the data based on the edit distance between strings, and then large numbers of strings can be searched for closest matches.</p> 

                <p>Other literature where I've found VP trees being used:
                <ul>
                    <li><a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-0934-8">Computational biology - Searching genome datasets for cancer diagnosis</a></li>
                    <li><a href="http://ceur-ws.org/Vol-256/submission_12.pdf">Image processing - Content based image indexing</a></li>
                    <li><a href="http://ieeexplore.ieee.org/document/5739700/">Databases - Indexing and searching in multimedia databases</a></li>
                    <li><a href="http://www1.cs.columbia.edu/CAVE/publications/pdfs/Kumar_ECCV08_2.pdf">Computer vision - Finding similar patches of images</a></li>
                    <li><a href="https://www.scholars.northwestern.edu/en/publications/speeding-melody-search-with-vantage-point-trees">Music - Speeding melody search in musical databases</a></li>
                    <li><a href="https://dspace.library.uu.nl/handle/1874/307362">Algorithms - Optimization of evolutionary algorithms</a></li>
                </ul>

                <h2>Demos</h2>

                
                <a href="https://fribbels.github.io/vptree/vptree.html?type=steps&strokeWidth=2&size=600&n=1000&dotSize=1.5">Build</a>
                                <!-- <pre><code class="javascript">
                


The cool thing about VP trees is that your specificallyce doesn't have to be Euclidean, i.e. you can have dimensions that depend on other dimensions, e.g. in RGBA color space values of RGB are meaningless when A=transparent. You can't search that space with KD trees, but you can with VP trees.
                 -->
            </div>
        </div>
    </body>
</html>